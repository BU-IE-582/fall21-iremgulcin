{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 – On the use of distance information for UwaveGesture Recognition Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns         \n",
    "from math import sqrt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import seaborn as sn\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read train set\n",
    "X_gesture_train = pd.read_csv('uWaveGestureLibrary_X_TRAIN', sep=\" \", prefix=\"t\", header=None, skipinitialspace=True)\n",
    "Y_gesture_train = pd.read_csv('uWaveGestureLibrary_Y_TRAIN', sep=\" \", prefix=\"t\", header=None, skipinitialspace=True)\n",
    "Z_gesture_train = pd.read_csv('uWaveGestureLibrary_Z_TRAIN', sep=\" \", prefix=\"t\", header=None, skipinitialspace=True)\n",
    "#read test set\n",
    "X_gesture_test = pd.read_csv('uWaveGestureLibrary_X_TEST', sep=\" \", prefix=\"t\", header=None, skipinitialspace=True)\n",
    "Y_gesture_test = pd.read_csv('uWaveGestureLibrary_Y_TEST', sep=\" \", prefix=\"t\", header=None, skipinitialspace=True)\n",
    "Z_gesture_test = pd.read_csv('uWaveGestureLibrary_Z_TEST', sep=\" \", prefix=\"t\", header=None, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged train set size= (896, 946)\n",
      "merged test set size= (3582, 946)\n"
     ]
    }
   ],
   "source": [
    "#rename columns name of train set\n",
    "X_gesture_train.columns = ['X{}'.format(i) for i in range(0,len(X_gesture_train.columns))]\n",
    "Y_gesture_train.columns = ['Y{}'.format(i) for i in range(0,len(Y_gesture_train.columns))]\n",
    "Z_gesture_train.columns = ['Z{}'.format(i) for i in range(0,len(Z_gesture_train.columns))]\n",
    "#merge x, y and z coordinates \n",
    "allxyz_train=pd.concat([X_gesture_train.iloc[:,:],Y_gesture_train.iloc[:,1:],Z_gesture_train.iloc[:,1:]], axis=1)\n",
    "print(\"merged train set size=\", allxyz_train.shape)\n",
    "\n",
    "#rename columns name of test set\n",
    "X_gesture_test.columns = ['X{}'.format(i) for i in range(0,len(X_gesture_test.columns))]\n",
    "Y_gesture_test.columns = ['Y{}'.format(i) for i in range(0,len(Y_gesture_test.columns))]\n",
    "Z_gesture_test.columns = ['Z{}'.format(i) for i in range(0,len(Z_gesture_test.columns))]\n",
    "#merge x, y and z coordinates \n",
    "allxyz_test=pd.concat([X_gesture_test.iloc[:,:],Y_gesture_test.iloc[:,1:],Z_gesture_test.iloc[:,1:]], axis=1)\n",
    "print(\"merged test set size=\", allxyz_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Suppose we decided to apply a nearest-neighbor (NN) classifier to find the labels of test\n",
    "instances. You can use the strategy you employed when you apply PCA to this data in\n",
    "Homework 2 (i.e. concatenation of the axes). Propose two distance measures for computing\n",
    "similarity between two time series. The distance calculation on the concatenated time series\n",
    "implicitly weights the distances of each axis in an equal way. For each distance measure\n",
    "alternative, use the training data to identify the ideal value of k which minimizes the error of a\n",
    "10-fold cross-validation.\n",
    "\n",
    "b) Using the value of k (identified for each distance measure) in part (a) and evaluate your final\n",
    "performance on the test data and present your results in a (8-by-8) confusion matrix, showing\n",
    "the counts for actual and predicted labels. In addition, quote the runtime and accuracy for your\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data as input and the output for both train and test set\n",
    "X_train=allxyz_train.iloc[:,1:]\n",
    "Y_train=allxyz_train.iloc[:,:1:]\n",
    "X_test=allxyz_test.iloc[:,1:]\n",
    "Y_test=allxyz_test.iloc[:,:1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k for euclidean= 3\n"
     ]
    }
   ],
   "source": [
    "#find ideal valu of k for euclidean metric\n",
    "k_values = [i for i in range(1,50,2)]\n",
    "k_acc=[]\n",
    "for i in k_values: \n",
    "    knn = KNeighborsClassifier(n_neighbors = i, metric = 'euclidean', n_jobs=-1)\n",
    "    cv_scores=cross_val_score(knn, X_train, np.ravel(Y_train), cv=10, scoring='accuracy')\n",
    "    k_acc.append(cv_scores.mean())\n",
    "optimal_k=k_values[k_acc.index(max(k_acc))]\n",
    "print(\"Optimal k for euclidean=\", format(optimal_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our confusion matrix is \n",
      "      0    1    2    3    4    5    6    7\n",
      "0  431    0    0    2    0    4    0    0\n",
      "1    1  449    0    0    0    0    2    0\n",
      "2    2    0  413    0   15   20    4    0\n",
      "3    7    0    0  370   60    6    0    7\n",
      "4    3    0    6    1  422    1    0    0\n",
      "5    6    0    7   15   28  392    1    0\n",
      "6    0    0    1    0    0    0  446    0\n",
      "7    0    0    0    1    1    0    0  458\n",
      "Our accuracy is 0.9438860971524288\n",
      "Duration: 0:00:04.635774\n"
     ]
    }
   ],
   "source": [
    "#for euclidean the optimum n_neigbors value is 3\n",
    "#find the confusion matrix and the accuracy for euclidean\n",
    "start_time = datetime.now()\n",
    "\n",
    "classifier_e = KNeighborsClassifier(n_neighbors = 3, metric = 'euclidean', p = 2)\n",
    "classifier_e.fit(X_train, np.ravel(Y_train))\n",
    "y_pred = classifier_e.predict(X_test)\n",
    "\n",
    "cm_e = confusion_matrix(Y_test, y_pred)\n",
    "ac_e = accuracy_score(Y_test,y_pred)\n",
    "print(\"Our confusion matrix is \\n\", pd.DataFrame(cm_e))\n",
    "print(\"Our accuracy is\", ac_e)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k for manhattan= 1\n"
     ]
    }
   ],
   "source": [
    "#find ideal valu of k for manhattan metric\n",
    "k_values = [i for i in range(1,50,2)]\n",
    "k_acc=[]\n",
    "for i in k_values: \n",
    "    knn = KNeighborsClassifier(n_neighbors = i, metric = 'manhattan', n_jobs=-1)\n",
    "    cv_scores=cross_val_score(knn, X_train, np.ravel(Y_train), cv=10, scoring='accuracy')\n",
    "    k_acc.append(cv_scores.mean())\n",
    "optimal_k=k_values[k_acc.index(max(k_acc))]\n",
    "print(\"Optimal k for manhattan=\", format(optimal_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function confusion_matrix at 0x0000021A59A30D90>\n",
      "Our confusion matrix is \n",
      "      0    1    2    3    4    5    6    7\n",
      "0  430    0    0    2    0    5    0    0\n",
      "1    1  451    0    0    0    0    0    0\n",
      "2    2    0  418    0   16   14    4    0\n",
      "3    3    0    0  392   40   11    0    4\n",
      "4    3    0    9    5  415    1    0    0\n",
      "5    3    0    5   13   16  411    0    1\n",
      "6    0    0    2    0    0    0  445    0\n",
      "7    0    0    0    3    1    0    0  456\n",
      "Our accuracy is 0.954215522054718\n",
      "Duration: 0:00:04.031079\n"
     ]
    }
   ],
   "source": [
    "#for manhattan the optimum n_neigbors value is 1\n",
    "#find the confusion matrix and the accuracy for manhattan\n",
    "start_time = datetime.now()\n",
    "\n",
    "classifier_m = KNeighborsClassifier(n_neighbors = 1, metric = 'manhattan', p = 2)\n",
    "classifier_m.fit(X_train, np.ravel(Y_train))\n",
    "y_pred = classifier_m.predict(X_test)\n",
    "\n",
    "cm_m = confusion_matrix(Y_test, y_pred)\n",
    "ac_m = accuracy_score(Y_test,y_pred)\n",
    "\n",
    "print (confusion_matrix)\n",
    "print(\"Our confusion matrix is \\n\", pd.DataFrame(cm_m))\n",
    "print(\"Our accuracy is\", ac_m)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) The observations from different axes are weighted equally if we compute the distance over each\n",
    "axis and sum them to obtain a final similarity measure. Is this reasonable? For example, we can\n",
    "compute the distance as below:\n",
    "\n",
    "\n",
    "where DistX is the distance based on the acceleration only on X axis, DistY is for Y axis and so\n",
    "on. Do you think weighting the distances over different axes to obtain a final similarity measure\n",
    "makes sense for classification? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) Yes, it makes sense. Calculating the distances over x and y is more reasonable because the z causes extra information. 2D calculation is enough that is why the weight of z is less effective in this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 – Linear models on alternative representations of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Class 3 to 1 and the others to 0\n",
    "Y_train3 = (allxyz_train[\"X0\"]==3)*1\n",
    "Y_test3 = (allxyz_test[\"X0\"]==3)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Train a logistic regression model on the training data and use the model to make a prediction on the\n",
    "test data. Note that you will obtain probabilistic predictions (i.e. probability of a time series being from\n",
    "Class 3 if you encoded Class 3 as 1 in binary classification setting). This will require you to select a\n",
    "threshold since 0.5 as a threshold may not work well under this imbalanced class setting. To make\n",
    "things easier, use the ratio of Class 3 instances in the training data as threshold. Use the learned model\n",
    "to predict the class for test data. Present your results in a (2-by-2) confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\90505\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train a logistic regression model on train set\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, np.ravel(Y_train3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the treshold\n",
    "th=Y_train3.sum()/len(X_train)\n",
    "th=float(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction on test set\n",
    "probs_y=model.predict_proba(X_test) \n",
    "probs_y=(probs_y[:,1]>th)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our confusion matrix is \n",
      "       0    1\n",
      "0  2828  300\n",
      "1    59  395\n"
     ]
    }
   ],
   "source": [
    "#2-by-2 confusion matrix\n",
    "conf_matrix = metrics.confusion_matrix(Y_test3, probs_y)\n",
    "print(\"Our confusion matrix is \\n\", pd.DataFrame(conf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is= 0.900\n"
     ]
    }
   ],
   "source": [
    "accuracy1=(conf_matrix[0][0]+conf_matrix[1][1])/conf_matrix.sum()\n",
    "print(\"Accuracy is= %.3f\" % accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) An advantage of logistic regression is related to the interpretability however when we have large\n",
    "number of features together with a method without penalization, it is harder to interpret the results.\n",
    "Therefore, an alternative way is to train a logistic regression model with lasso penalties. This will\n",
    "require you setting of penalization term (namely lambda). Use 10-fold cross-validation to determine\n",
    "your ideal lambda level based on binomial deviance (Note that we have used accuracy as primary\n",
    "metric to determine the lambda in class, however this strategy may not work well for the imbalanced\n",
    "data). You can check http://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/MLSC_Lec4.pdf\n",
    "for details of binomial deviance. This is also referred to as logistic loss. If you are using “glmnet”\n",
    "package in R, “type.measure” can be set to “deviance” which is the default value. If you are Python\n",
    "user, sklearn module has “LogisticRegressionCV” function in which you can provide the scorer as\n",
    "“metrics.log_loss”.\n",
    "Once you determine your best lambda value using 10-fold cross-validation, perform classification on\n",
    "test data similar to part a and compare your results. Comment on the regression coefficients. Is there\n",
    "any interesting information? Try to interpret the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train logistic regression model with lasso penalties\n",
    "model_lasso = LogisticRegressionCV(cv=10, penalty=\"l1\", max_iter=1000, scoring='neg_mean_absolute_error', solver='liblinear')\n",
    "model_lasso.fit(X_train, np.ravel(Y_train3))\n",
    "pred_y_lasso = model_lasso.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score= 2.863827716750354\n"
     ]
    }
   ],
   "source": [
    "#calculate the loss due to the chosen lambda values by the 10-fold cross validation above\n",
    "pred_y_lasso = (pred_y_lasso[:,1]>th)*1\n",
    "score = metrics.log_loss(Y_test3, pred_y_lasso)\n",
    "print(\"best score=\", score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our confusion matrix is \n",
      "       0    1\n",
      "0  2860  268\n",
      "1    29  425\n"
     ]
    }
   ],
   "source": [
    "#2-by-2 confusion matrix for LR lasso model\n",
    "conf_matrix_lasso = metrics.confusion_matrix(Y_test3, pred_y_lasso)\n",
    "print(\"Our confusion matrix is \\n\", pd.DataFrame(conf_matrix_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is= 0.917\n"
     ]
    }
   ],
   "source": [
    "#accuracy calculation\n",
    "accuracy2=(conf_matrix_lasso[0][0]+conf_matrix_lasso[1][1])/conf_matrix_lasso.sum()\n",
    "print(\"Accuracy is= %.3f\" % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  35\n",
      "y =  19\n",
      "z =  10\n",
      "sumx =  2.510454815309821\n",
      "sumy =  1.4170113139440206\n",
      "sumz =  -0.4088124865978635\n"
     ]
    }
   ],
   "source": [
    "xyz = pd.DataFrame((model_lasso.coef_))\n",
    "x1 = xyz.iloc[:,:315]\n",
    "x = ((x1.to_numpy()!=0)*1).sum()\n",
    "sumx = x1.to_numpy().sum()\n",
    "y1 = xyz.iloc[:,315:630]\n",
    "y = ((y1.to_numpy()!=0)*1).sum()\n",
    "sumy = y1.to_numpy().sum()\n",
    "z1 = xyz.iloc[:,630:]\n",
    "z = ((z1.to_numpy()!=0)*1).sum()\n",
    "sumz = z1.to_numpy().sum()\n",
    "print(\"x = \", x)\n",
    "print(\"y = \", y)\n",
    "print(\"z = \", z)\n",
    "print(\"sumx = \", sumx)\n",
    "print(\"sumy = \", sumy)\n",
    "print(\"sumz = \", sumz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the coefficients are interpreted, it can be seen that X axis is the most effective one by it is possitive coefficients. It has the most non zero values also. X direction is the significant one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) An alternative way to represent the feature matrix on a new space to introduce nonlinear relations is\n",
    "to use distance matrix as a feature matrix. For example, we have 896 training instances and the\n",
    "observations over time are used as features in the previous tasks (i.e. we worked on N by 3T matrix).\n",
    "Recall that multidimensional scaling also works on distance matrices and we have mentioned that it can\n",
    "handle nonlinear relations (Homework 2 also aims at revealing such an information). This non-linearity\n",
    "stems from the use of Euclidean distances. Use of Euclidean distance as input to a learning algorithm\n",
    "allows for handling nonlinear relations*. In other words, your features keep the nonlinear information.\n",
    "*We will have further discussion on this behavior when we cover support vector machines. This type of transformations are\n",
    "discussed under “distance-based kernels” which is out of scope for now. Additional information is provided in case you are\n",
    "willing to perform research on distance based transformations.\n",
    "Given this information, you are expected to transform your training data to distance information (i.e. N\n",
    "by N matrix). Note that you need to perform a similar transformation to your test data. In other words,\n",
    "you need to calculate the distance of each test instance to training instance to obtain a distance based\n",
    "representation for your test data. This will be an Ntest by N matrix (Ntest refers to the number of test\n",
    "instances) where each entry (i,j) refers to the distance of test time series i to the training time series j.\n",
    "You can use Euclidean distance as your distance measure.\n",
    "Perform the same training and test strategy as in part b but use the distances as your new feature\n",
    "matrices. Comment on the regression coefficients. What do they imply under this new representation\n",
    "setting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>886</th>\n",
       "      <th>887</th>\n",
       "      <th>888</th>\n",
       "      <th>889</th>\n",
       "      <th>890</th>\n",
       "      <th>891</th>\n",
       "      <th>892</th>\n",
       "      <th>893</th>\n",
       "      <th>894</th>\n",
       "      <th>895</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.687559</td>\n",
       "      <td>13.263419</td>\n",
       "      <td>28.321555</td>\n",
       "      <td>39.541255</td>\n",
       "      <td>34.286788</td>\n",
       "      <td>38.211025</td>\n",
       "      <td>36.045863</td>\n",
       "      <td>48.764229</td>\n",
       "      <td>37.568076</td>\n",
       "      <td>59.428712</td>\n",
       "      <td>...</td>\n",
       "      <td>44.366890</td>\n",
       "      <td>49.494730</td>\n",
       "      <td>32.777330</td>\n",
       "      <td>51.240891</td>\n",
       "      <td>35.257192</td>\n",
       "      <td>43.642435</td>\n",
       "      <td>51.738237</td>\n",
       "      <td>41.011374</td>\n",
       "      <td>41.870528</td>\n",
       "      <td>28.349480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.947350</td>\n",
       "      <td>54.334337</td>\n",
       "      <td>54.694516</td>\n",
       "      <td>46.732487</td>\n",
       "      <td>45.800128</td>\n",
       "      <td>46.545913</td>\n",
       "      <td>41.582900</td>\n",
       "      <td>37.517528</td>\n",
       "      <td>46.173264</td>\n",
       "      <td>31.870273</td>\n",
       "      <td>...</td>\n",
       "      <td>37.760887</td>\n",
       "      <td>42.294340</td>\n",
       "      <td>44.207834</td>\n",
       "      <td>38.800478</td>\n",
       "      <td>50.753346</td>\n",
       "      <td>42.983088</td>\n",
       "      <td>38.362504</td>\n",
       "      <td>44.871331</td>\n",
       "      <td>37.517618</td>\n",
       "      <td>55.648530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.171916</td>\n",
       "      <td>25.570757</td>\n",
       "      <td>35.860929</td>\n",
       "      <td>42.167440</td>\n",
       "      <td>28.760080</td>\n",
       "      <td>37.984422</td>\n",
       "      <td>41.091221</td>\n",
       "      <td>44.311017</td>\n",
       "      <td>31.095472</td>\n",
       "      <td>50.164764</td>\n",
       "      <td>...</td>\n",
       "      <td>46.952663</td>\n",
       "      <td>50.504509</td>\n",
       "      <td>30.280558</td>\n",
       "      <td>39.453718</td>\n",
       "      <td>45.695170</td>\n",
       "      <td>47.538985</td>\n",
       "      <td>50.386797</td>\n",
       "      <td>40.439901</td>\n",
       "      <td>43.688492</td>\n",
       "      <td>35.511051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.876963</td>\n",
       "      <td>34.269027</td>\n",
       "      <td>41.884193</td>\n",
       "      <td>44.840555</td>\n",
       "      <td>25.517543</td>\n",
       "      <td>43.336345</td>\n",
       "      <td>38.773427</td>\n",
       "      <td>38.422948</td>\n",
       "      <td>20.204116</td>\n",
       "      <td>50.381287</td>\n",
       "      <td>...</td>\n",
       "      <td>45.197983</td>\n",
       "      <td>53.934857</td>\n",
       "      <td>28.368976</td>\n",
       "      <td>42.632988</td>\n",
       "      <td>48.775380</td>\n",
       "      <td>49.697657</td>\n",
       "      <td>44.197285</td>\n",
       "      <td>42.866915</td>\n",
       "      <td>42.667953</td>\n",
       "      <td>41.614170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.642237</td>\n",
       "      <td>14.399477</td>\n",
       "      <td>16.490769</td>\n",
       "      <td>41.308488</td>\n",
       "      <td>32.737889</td>\n",
       "      <td>37.112181</td>\n",
       "      <td>44.964582</td>\n",
       "      <td>47.155949</td>\n",
       "      <td>37.856534</td>\n",
       "      <td>57.156578</td>\n",
       "      <td>...</td>\n",
       "      <td>47.806687</td>\n",
       "      <td>46.909740</td>\n",
       "      <td>32.783804</td>\n",
       "      <td>49.979809</td>\n",
       "      <td>34.794183</td>\n",
       "      <td>40.932294</td>\n",
       "      <td>53.618109</td>\n",
       "      <td>46.638917</td>\n",
       "      <td>47.715370</td>\n",
       "      <td>22.199250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 896 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0  51.687559  13.263419  28.321555  39.541255  34.286788  38.211025   \n",
       "1  34.947350  54.334337  54.694516  46.732487  45.800128  46.545913   \n",
       "2  42.171916  25.570757  35.860929  42.167440  28.760080  37.984422   \n",
       "3  41.876963  34.269027  41.884193  44.840555  25.517543  43.336345   \n",
       "4  49.642237  14.399477  16.490769  41.308488  32.737889  37.112181   \n",
       "\n",
       "         6          7          8          9    ...        886        887  \\\n",
       "0  36.045863  48.764229  37.568076  59.428712  ...  44.366890  49.494730   \n",
       "1  41.582900  37.517528  46.173264  31.870273  ...  37.760887  42.294340   \n",
       "2  41.091221  44.311017  31.095472  50.164764  ...  46.952663  50.504509   \n",
       "3  38.773427  38.422948  20.204116  50.381287  ...  45.197983  53.934857   \n",
       "4  44.964582  47.155949  37.856534  57.156578  ...  47.806687  46.909740   \n",
       "\n",
       "         888        889        890        891        892        893  \\\n",
       "0  32.777330  51.240891  35.257192  43.642435  51.738237  41.011374   \n",
       "1  44.207834  38.800478  50.753346  42.983088  38.362504  44.871331   \n",
       "2  30.280558  39.453718  45.695170  47.538985  50.386797  40.439901   \n",
       "3  28.368976  42.632988  48.775380  49.697657  44.197285  42.866915   \n",
       "4  32.783804  49.979809  34.794183  40.932294  53.618109  46.638917   \n",
       "\n",
       "         894        895  \n",
       "0  41.870528  28.349480  \n",
       "1  37.517618  55.648530  \n",
       "2  43.688492  35.511051  \n",
       "3  42.667953  41.614170  \n",
       "4  47.715370  22.199250  \n",
       "\n",
       "[5 rows x 896 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creation of feature matrix by the euclidean distance\n",
    "train = euclidean_distances(X_train)\n",
    "train = pd.DataFrame(train)\n",
    "test = euclidean_distances(X_test, X_train)\n",
    "test = pd.DataFrame(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train logistic regression model with lasso penalties\n",
    "model_lasso_2 = LogisticRegressionCV(cv=10, penalty=\"l1\", max_iter=1000, scoring='neg_mean_absolute_error', solver='liblinear')\n",
    "model_lasso_2.fit(train, np.ravel(Y_train3))\n",
    "pred_y_lasso_2 = model_lasso_2.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score= 0.4917641499919217\n"
     ]
    }
   ],
   "source": [
    "#calculate the loss due to the chosen lambda values by the 10-fold cross validation above\n",
    "pred_y_lasso_2 = (pred_y_lasso_2[:,1]>th)*1\n",
    "score = metrics.log_loss(Y_test3, pred_y_lasso_2)\n",
    "print(\"best score=\", score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our confusion matrix is \n",
      "       0    1\n",
      "0  3101   27\n",
      "1    24  430\n"
     ]
    }
   ],
   "source": [
    "#2-by-2 confusion matrix\n",
    "conf_matrix_lasso_2 = metrics.confusion_matrix(Y_test3, pred_y_lasso_2)\n",
    "print(\"Our confusion matrix is \\n\", pd.DataFrame(conf_matrix_lasso_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is= 0.986\n"
     ]
    }
   ],
   "source": [
    "accuracy3=(conf_matrix_lasso_2[0][0]+conf_matrix_lasso_2[1][1])/conf_matrix_lasso_2.sum()\n",
    "print(\"Accuracy is= %.3f\" % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xyz =  889\n",
      "sumxyz =  -0.23553478634436253\n"
     ]
    }
   ],
   "source": [
    "xyz1 = pd.DataFrame((model_lasso_2.coef_))\n",
    "xyz = ((xyz1.to_numpy()!=0)*1).sum()\n",
    "sumxyz = (xyz1.to_numpy()).sum()\n",
    "print(\"xyz = \", xyz)\n",
    "print(\"sumxyz = \", sumxyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 889 non zero entries and the number of positive output is much more less. The only decision for the model is to understand the class is 3 or not so the valu of the outpu is not important and the model gives a very good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Provide an overall comparison on the results you obtain for each part (over all tasks). You can\n",
    "compare test accuracy of each alternative method you developed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies from beginning to the end are =\n",
      "0.900 \n",
      "0.917\n",
      "0.986\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracies from beginning to the end are =\\n%.3f \" %accuracy1)\n",
    "print(\"%.3f\" %accuracy2)\n",
    "print(\"%.3f\" %accuracy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression with lasso is better than the logisctic regression model. By the non linear features and lasso penalization, model becomes much better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
